{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/audal/lib/python3.8/site-packages/treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "/home/ubuntu/anaconda3/envs/audal/lib/python3.8/site-packages/treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "/home/ubuntu/anaconda3/envs/audal/lib/python3.8/site-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "/home/ubuntu/anaconda3/envs/audal/lib/python3.8/site-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import time\n",
    "from bson.json_util import dumps\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import treetaggerwrapper\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "from time import time, ctime\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from scipy import spatial\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connection to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Server\n",
    "server = MongoClient('')\n",
    "#BD\n",
    "db = server['aura_pmi_dl']\n",
    "#Collection\n",
    "collection = db['document_profiles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connection to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://159.84.108.111:7687\", auth=(\"neo4j\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connection to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FR\n",
    "stpw_fr = list(set(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EN\n",
    "stpw_en = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_fr = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_en = treetaggerwrapper.TreeTagger(TAGLANG='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data in french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8649 Hits:\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "##### DATA IN FRENCH\n",
    "\n",
    "\n",
    "#First query to check number of hits\n",
    "query = {\"query\": {\"match_all\": {}}}\n",
    "res = es.search(index=\"document_index_fr\", body=query, scroll='2m',)\n",
    "result_size = res['hits']['total']['value']\n",
    "print(\"Got %d Hits:\" % result_size)\n",
    "# Get the scroll ID\n",
    "first = True\n",
    "sid = res['_scroll_id']\n",
    "scroll_size = len(res['hits']['hits'])\n",
    "\n",
    "liste_text_fr = []\n",
    "liste_id_fr = []    \n",
    "while ((scroll_size > 0) | first):   \n",
    "    \n",
    "    for hit in res['hits']['hits']:\n",
    "        \n",
    "        identifier = hit['_source']['identifier']\n",
    "        text_fr = hit['_source']['content']\n",
    "        liste_id_fr.append(identifier)\n",
    "    \n",
    "    \n",
    "        #PROCESSING THE RESULT\n",
    "        \n",
    "        #Stopwords removal\n",
    "        word_tokens = word_tokenize(text_fr, language='french') \n",
    "        filtered_text = [w for w in word_tokens if w not in stpw_fr] \n",
    "        filtered_text =[term.lower() for term in filtered_text if term.isalpha()]\n",
    "        filtered_text_fr = \" \".join(filtered_text)\n",
    "       \n",
    "        #Lemmatization\n",
    "       \n",
    "        text = filtered_text_fr.lower()\n",
    "        tags = tagger_fr.tag_text(text)\n",
    "        liste_lemmas = [tag.split('\\t')[2] for tag in tags]\n",
    "        lemmatized_text_fr = \" \".join(liste_lemmas)\n",
    "        liste_text_fr.append(lemmatized_text_fr)    \n",
    "    \n",
    "    ##Query again\n",
    "    res = es.scroll(scroll_id=sid, scroll='2m')\n",
    "    # Update the scroll ID\n",
    "    first = False\n",
    "    sid = res['_scroll_id']\n",
    "    scroll_size = len(res['hits']['hits'])\n",
    "    \n",
    "print(\"END\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 41324 Hits:\n"
     ]
    }
   ],
   "source": [
    "##### DATA IN ENGLISH\n",
    "\n",
    "\n",
    "#First query to check number of hits\n",
    "query = {\"query\": {\"match_all\": {}}}\n",
    "res = es.search(index=\"document_index_en\", body=query, scroll='2m',)\n",
    "result_size = res['hits']['total']['value']\n",
    "print(\"Got %d Hits:\" % result_size)\n",
    "# Get the scroll ID\n",
    "first = True\n",
    "sid = res['_scroll_id']\n",
    "scroll_size = len(res['hits']['hits'])\n",
    "\n",
    "liste_text_en = []\n",
    "liste_id_en = []    \n",
    "while ((scroll_size > 0) | first):   \n",
    "    \n",
    "    for hit in res['hits']['hits']:\n",
    "        \n",
    "        identifier = hit['_source']['identifier']\n",
    "        text_en = hit['_source']['content']\n",
    "        liste_id_en.append(identifier)\n",
    "    \n",
    "    \n",
    "        #PROCESSING THE RESULT\n",
    "        \n",
    "        #Stopwords removal\n",
    "        word_tokens = word_tokenize(text_en, language='english') \n",
    "        filtered_text = [w for w in word_tokens if w not in stpw_en] \n",
    "        filtered_text =[term.lower() for term in filtered_text if term.isalpha()]\n",
    "        filtered_text_en = \" \".join(filtered_text)\n",
    "       \n",
    "        #Lemmatization\n",
    "       \n",
    "        text = filtered_text_en.lower()\n",
    "        tags = tagger_en.tag_text(text)\n",
    "        liste_lemmas = [tag.split('\\t')[2] for tag in tags]\n",
    "        lemmatized_text_en = \" \".join(liste_lemmas)\n",
    "        liste_text_en.append(lemmatized_text_en)    \n",
    "    \n",
    "    ##Query again\n",
    "    res = es.scroll(scroll_id=sid, scroll='2m')\n",
    "    # Update the scroll ID\n",
    "    first = False\n",
    "    sid = res['_scroll_id']\n",
    "    scroll_size = len(res['hits']['hits'])\n",
    "    \n",
    "print(\"END\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization - Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liste_text_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-964e10f0326f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mliste_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mliste_id_fr\u001b[0m \u001b[0;31m#+ liste_id_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mliste_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mliste_text_fr\u001b[0m \u001b[0;31m#+ liste_text_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mliste_text_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mliste_id_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mliste_text_fr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'liste_text_en' is not defined"
     ]
    }
   ],
   "source": [
    "#Global lists\n",
    "liste_id = liste_id_fr + liste_id_en\n",
    "liste_text = liste_text_fr + liste_text_en\n",
    "del liste_text_en\n",
    "del liste_id_en\n",
    "del liste_text_fr\n",
    "del liste_id_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_COMPONENTS = 100\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(liste_text)]\n",
    "model = Doc2Vec(documents, vector_size=NB_COMPONENTS, window=3, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"my_doc2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(fname)  # you can continue training with the loaded model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation of representations in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Creation of representation nodes\n",
    "for identifier in liste_id:\n",
    "    graph.run('CREATE (c:Document:Refined {identifier:\"'+ identifier +'\", vocabulary:\"global_vocabulary\", format:\"embedding\"}) RETURN c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Linking representation nodes with main documents\n",
    "query = '''MATCH (d:Object),(p:Refined {vocabulary:\"global_vocabulary\", format:\"embedding\"}) WHERE d.identifier = p.identifier\n",
    "            CREATE (d)-[r:REPRESENTATION]->(p)\n",
    "            RETURN d,p'''\n",
    "res = graph.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Generation of similarities\n",
    "\n",
    "#scc_similarity_matrix = spearmanr(liste_doc_vectors, liste_doc_vectors)\n",
    "#cos_similarity_matrix = cosine_similarity(liste_doc_vectors, liste_doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation of representation in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Adding the \"representations field\"\n",
    "for i, identifier in enumerate(liste_id):\n",
    "    identifier = liste_id[i]\n",
    "    dimensions = np.array(['dim'+str(i) for i in range(NB_COMPONENTS)])\n",
    "    doc_vocab = pd.DataFrame({\"dim\":dimensions, \"value\":model.docvecs[i]}) \n",
    "    doc_vocab.index = doc_vocab[\"dim\"]\n",
    "    representation = doc_vocab.to_dict(orient='records') #Generation of the representation\n",
    "    query = {'_id': identifier}\n",
    "    new_values = {'$addToSet': {'representations': {\"vocabulary\":\"global_vocabulary\", \"format\":\"embedding\", \"data\":representation} }}\n",
    "    collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation of Similarity Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1273403 , -0.14608477,  0.4224325 ,  0.04118891,  0.76641405,\n",
       "       -0.17481771,  1.2649163 , -1.1716287 ,  0.90775836, -0.8898628 ,\n",
       "        0.5827632 , -1.5164006 , -0.07379433,  0.4499898 ,  0.07833008,\n",
       "       -0.48350087,  0.68391824, -0.21687557, -0.20227014, -1.0840961 ,\n",
       "        0.67306376, -0.24656947, -0.60194266,  0.3168104 , -1.2110342 ,\n",
       "       -0.35495126, -0.561662  , -0.5264341 , -0.65603584,  0.84768075,\n",
       "        0.625523  ,  0.54525894,  0.85951424, -0.61103547, -0.79932   ,\n",
       "       -0.32540563, -0.1675622 , -1.2314767 , -0.15207194, -0.21316959,\n",
       "       -1.1078626 , -0.716714  , -0.8542841 , -0.18060179, -0.93470615,\n",
       "        0.15863393, -1.647496  ,  0.979502  , -0.12137932, -0.29938632,\n",
       "        0.9153041 , -0.71219724, -0.33108902,  0.8375322 , -0.05632454,\n",
       "        0.29518408,  0.16091841,  0.1025452 ,  0.26846913, -0.81526554,\n",
       "       -0.43847927,  0.08741318,  0.04842928,  0.93069994, -0.26138   ,\n",
       "        0.909917  , -0.7010395 ,  0.19297259,  0.3426578 ,  0.080215  ,\n",
       "        0.11636709,  0.01877464,  0.04921652, -0.31511143, -0.23544946,\n",
       "       -0.49432155,  0.02495801, -0.22088663,  0.31293055,  0.33181116,\n",
       "        0.96887696,  0.36991107,  0.38412076,  0.42979312,  0.04585575,\n",
       "        0.31397054,  0.59825945,  0.03696801, -0.57634723,  0.07467553,\n",
       "       -0.38807964,  0.48861498,  0.3878321 ,  0.6281043 ,  1.6393276 ,\n",
       "        0.1650143 , -0.74586356, -0.03915963, -1.0185328 ,  0.20356269],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer_vector = model.infer_vector(liste_text[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Load_model('model.doc2vec')\n",
    "\n",
    "#infer_vector = model.infer_vector(s)\n",
    "\n",
    "#similar_documents = model.docvecs.most_similar([model.docvecs[0]], topn = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9999998807907104),\n",
       " (8331, 0.7180832624435425),\n",
       " (128, 0.6914499998092651),\n",
       " (2746, 0.6743021011352539),\n",
       " (1188, 0.6647195816040039)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999998807907104"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similar_documents[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 5\n",
    "seuil=0.60\n",
    "k=0\n",
    "l=0\n",
    "for i, identifier1 in enumerate(liste_id):\n",
    "    similar_documents = model.docvecs.most_similar([model.docvecs[i]], topn = nb+1)\n",
    "    for j in range(1, nb+1):\n",
    "        num = similar_documents[j][0]\n",
    "        sim = similar_documents[j][1]\n",
    "        identifier2 = liste_id[num]\n",
    "        if(sim >= seuil):#We only consider similarities higher than a certain threshold\n",
    "            query = '''MATCH (r1:Refined {vocabulary:\"global_vocabulary\", identifier:\"'''+ identifier1 +'''\", format:\"embedding\"}),\n",
    "            (r2:Refined {vocabulary:\"global_vocabulary\", identifier:\"'''+ identifier2 +'''\", format:\"embedding\"})\n",
    "                CREATE (r1)<-[r:SIMILARITY {measure: \"cosine_similarity\", value:''' + str(sim) + '''}]-(r2)\n",
    "                RETURN r1,r2'''\n",
    "            graph.run(query)\n",
    "            l +=1\n",
    "    if i % 1000 == 0:\n",
    "        print(\"i = \", str(i) ,\" time: \", ctime(time()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = collection.create_index({\"representations.vocabulary\":-1})\n",
    "    print(\"Index created on vocabulary\")\n",
    "except:\n",
    "    print(\"unable to create index on vocabulary in MongoDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = collection.create_index([(\"representations.vocabulary\",1),(\"representations.format\",1)])\n",
    "    print(\"Index created on format\")\n",
    "except:\n",
    "    print(\"unable to create index on format in MongoDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = collection.create_index([(\"representations.data.term\",1)])\n",
    "    print(\"Index created on term\")\n",
    "except:\n",
    "    print(\"unable to create index on term in MongoDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = collection.create_index([(\"representations.data.dim\",1)])\n",
    "    print(\"Index created on dim\")\n",
    "except:\n",
    "    print(\"unable to create index on dim in MongoDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del liste_id\n",
    "del liste_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = time()\n",
    "elapsed = done - start\n",
    "print(\"*\"*20)\n",
    "print(\"STARTED : \"+ctime(start) )\n",
    "print(\"END : \"+ctime(done) )\n",
    "print(\"TIME ELAPSED:\" + str(elapsed/60) + \" MINUTES\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
