{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/audal/lib/python3.8/site-packages/treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "/home/ubuntu/anaconda3/envs/audal/lib/python3.8/site-packages/treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "/home/ubuntu/anaconda3/envs/audal/lib/python3.8/site-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "/home/ubuntu/anaconda3/envs/audal/lib/python3.8/site-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import time\n",
    "from bson.json_util import dumps\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import treetaggerwrapper\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "from time import time, ctime\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connection to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Server\n",
    "server = MongoClient('')\n",
    "#BD\n",
    "db = server['aura_pmi_dl']\n",
    "#Collection\n",
    "collection = db['document_profiles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connection to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://host:7687\", auth=(\"neo4j\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connection to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"host\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FR\n",
    "with open('/home/ubuntu/aura-pmi/Ressources/french_stopwords.txt', 'r', encoding=\"utf8\") as f:\n",
    "    custom_stpw_fr = f.read().split()\n",
    "nltk_stopwords_fr = stopwords.words(\"french\")\n",
    "stpw_fr = set(custom_stpw_fr+nltk_stopwords_fr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EN\n",
    "stpw_en = list(set(stopwords.words('english')))\n",
    "with open('/home/ubuntu/aura-pmi/Ressources/english_stopwords.txt', 'r', encoding=\"utf8\") as f:\n",
    "    custom_stpw_en = f.read().split()\n",
    "nltk_stopwords_en = stopwords.words(\"english\")\n",
    "stpw_en = set(custom_stpw_en+nltk_stopwords_en) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_fr = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_en = treetaggerwrapper.TreeTagger(TAGLANG='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data in french\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8649 Hits:\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "##### DATA IN FRENCH\n",
    "\n",
    "\n",
    "#First query to check number of hits\n",
    "query = {\"query\": {\"match_all\": {}}}\n",
    "res = es.search(index=\"document_index_fr\", body=query, scroll='2m',)\n",
    "result_size = res['hits']['total']['value']\n",
    "print(\"Got %d Hits:\" % result_size)\n",
    "# Get the scroll ID\n",
    "first = True\n",
    "sid = res['_scroll_id']\n",
    "scroll_size = len(res['hits']['hits'])\n",
    "\n",
    "\n",
    "while ((scroll_size > 0) | first):\n",
    "  \n",
    "    liste_text_fr = []\n",
    "    liste_id_fr = []\n",
    "    \n",
    "    for hit in res['hits']['hits']:\n",
    "        \n",
    "        identifier = hit['_source']['identifier']\n",
    "        text_fr = hit['_source']['content']\n",
    "        liste_text_fr.append(text_fr)\n",
    "        liste_id_fr.append(identifier)\n",
    "    \n",
    "    liste_lemmatized_text_fr = []\n",
    "    for text_fr, id_fr in zip(liste_text_fr, liste_id_fr):\n",
    "        \n",
    "        #PROCESSING THE RESULT\n",
    "        \n",
    "        #Stopwords removal\n",
    "        word_tokens = word_tokenize(text_fr, language='french') \n",
    "        filtered_text = [w for w in word_tokens if w not in stpw_fr] \n",
    "        filtered_text =[term.lower() for term in filtered_text if term.isalpha()]\n",
    "        filtered_text_fr = \" \".join(filtered_text)\n",
    "       \n",
    "        #Lemmatization\n",
    "        \n",
    "        text = filtered_text_fr.lower()\n",
    "        tags = tagger_fr.tag_text(text)\n",
    "        #liste_lemmas = [tag.split('\\t')[2] for tag in tags]\n",
    "        liste_lemmas = []\n",
    "        for tag in tags: \n",
    "            tag_splited = tag.split('\\t')\n",
    "            if len(tag_splited) > 2:\n",
    "                if tag_splited[1] not in [\"DET:ART\", \"PRP\", \"KON\", \"PRP:det\"]: #First punctuation and articles filtering\n",
    "                    liste_lemmas.append(tag_splited[2])\n",
    "        lemmatized_text_fr = \" \".join(liste_lemmas)\n",
    "        liste_lemmatized_text_fr.append(lemmatized_text_fr)    \n",
    "\n",
    "        \n",
    "    #VECTORIZATION\n",
    "    vectorizer = CountVectorizer(max_features=1000)\n",
    "    mdt = vectorizer.fit_transform(liste_lemmatized_text_fr)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    vectors = mdt.toarray()\n",
    "    #Insertion in Neo4j and MongoDB\n",
    "    for i, identifier in enumerate(liste_id_fr):\n",
    "       \n",
    "        #Insertion in Neo4J\n",
    "        graph.run('CREATE (c:Document:Refined {identifier:\"'+ identifier +'\", vocabulary:\"global_vocabulary\", format:\"key-values\"}) RETURN c')\n",
    "        ##### Linking representation nodes with main documents\n",
    "        query = '''MATCH (d:Object),(p:Refined {name:\"global_vocabulary\", format:\"key-values\"}) WHERE d.identifier = p.identifier\n",
    "                    CREATE (d)-[r:REPRESENTATION]->(p)\n",
    "                    RETURN d,p'''\n",
    "        graph.run(query)\n",
    "    \n",
    "        #Insertion in MongoDB\n",
    "        doc_vector = vectors[i]\n",
    "        doc_vocab = pd.DataFrame({\"term\":vocabulary, \"freq\":doc_vector}) \n",
    "        doc_vocab.index = doc_vocab[\"term\"]\n",
    "        #print(doc_vocab.head())\n",
    "        doc_vocab = doc_vocab[doc_vocab['freq'] > 0]\n",
    "        representation = doc_vocab.to_dict(orient='records') #Generation of the representation\n",
    "        query = {'_id': identifier}\n",
    "        new_values = {'$addToSet': {'representations': {\"vocabulary\":\"global_vocabulary\", \"format\":\"key-values\", \"data\":representation} }}\n",
    "        collection.update_one(query, new_values)\n",
    "    \n",
    "    ##Query again\n",
    "    res = es.scroll(scroll_id=sid, scroll='2m')\n",
    "    # Update the scroll ID\n",
    "    first = False\n",
    "    sid = res['_scroll_id']\n",
    "    scroll_size = len(res['hits']['hits'])\n",
    "    \n",
    "print(\"END\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 41324 Hits:\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "##### DATA IN ENGLISH\n",
    "\n",
    "\n",
    "#First query to check number of hits\n",
    "query = {\"query\": {\"match_all\": {}}}\n",
    "res = es.search(index=\"document_index_en\", body=query, scroll='2m',)\n",
    "result_size = res['hits']['total']['value']\n",
    "print(\"Got %d Hits:\" % result_size)\n",
    "# Get the scroll ID\n",
    "first = True\n",
    "sid = res['_scroll_id']\n",
    "scroll_size = len(res['hits']['hits'])\n",
    "\n",
    "\n",
    "while ((scroll_size > 0) | first):\n",
    "  \n",
    "    liste_text_en = []\n",
    "    liste_id_en = []\n",
    "    \n",
    "    for hit in res['hits']['hits']:\n",
    "        \n",
    "        identifier = hit['_source']['identifier']\n",
    "        text_en = hit['_source']['content']\n",
    "        liste_text_en.append(text_en)\n",
    "        liste_id_en.append(identifier)\n",
    "    \n",
    "    liste_lemmatized_text_en = []\n",
    "    for text_en, id_en in zip(liste_text_en, liste_id_en):\n",
    "        \n",
    "        #PROCESSING THE RESULT\n",
    "        \n",
    "        #Stopwords removal\n",
    "        word_tokens = word_tokenize(text_fr, language='english') \n",
    "        filtered_text = [w for w in word_tokens if w not in stpw_en] \n",
    "        filtered_text =[term.lower() for term in filtered_text if term.isalpha()]\n",
    "        filtered_text_en = \" \".join(filtered_text)\n",
    "       \n",
    "        #Lemmatization\n",
    "       \n",
    "        text = filtered_text_en.lower()\n",
    "        tags = tagger_en.tag_text(text)\n",
    "        #liste_lemmas = [tag.split('\\t')[2] for tag in tags]\n",
    "        liste_lemmas = []\n",
    "        for tag in tags: \n",
    "            tag_splited = tag.split('\\t')\n",
    "            if len(tag_splited) > 2:\n",
    "                if tag_splited[1] not in [\"DET:ART\", \"PRP\", \"KON\", \"PRP:det\"]: #First punctuation and articles filtering\n",
    "                    liste_lemmas.append(tag_splited[2])\n",
    "        lemmatized_text_en = \" \".join(liste_lemmas)\n",
    "        liste_lemmatized_text_en.append(lemmatized_text_en)    \n",
    "\n",
    "        \n",
    "    #VECTORIZATION\n",
    "    vectorizer = CountVectorizer(max_features=1000)\n",
    "    mdt = vectorizer.fit_transform(liste_lemmatized_text_en)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    vectors = mdt.toarray()\n",
    "    #Insertion in Neo4j and MongoDB\n",
    "    for i, identifier in enumerate(liste_id_en):\n",
    "       \n",
    "        #Insertion in Neo4J\n",
    "        graph.run('CREATE (c:Document:Refined {identifier:\"'+ identifier +'\", vocabulary:\"global_vocabulary\", format:\"key-values\"}) RETURN c')\n",
    "        ##### Linking representation nodes with main documents\n",
    "        query = '''MATCH (d:Object),(p:Refined {name:\"alternative_vocabulary\"}) WHERE d.identifier = p.identifier\n",
    "                    CREATE (d)-[r:REPRESENTATION]->(p)\n",
    "                    RETURN d,p'''\n",
    "        graph.run(query)\n",
    "    \n",
    "        #Insertion in MongoDB\n",
    "        doc_vector = vectors[i]\n",
    "        doc_vocab = pd.DataFrame({\"term\":vocabulary, \"freq\":doc_vector}) \n",
    "        doc_vocab.index = doc_vocab[\"term\"]\n",
    "        #print(doc_vocab.head())\n",
    "        doc_vocab = doc_vocab[doc_vocab['freq'] > 0]\n",
    "        representation = doc_vocab.to_dict(orient='records') #Generation of the representation\n",
    "        query = {'_id': identifier}\n",
    "        new_values = {'$addToSet': {'representations': {\"vocabulary\":\"global_vocabulary\", \"format\":\"key-values\", \"data\":representation} }}\n",
    "        collection.update_one(query, new_values)\n",
    "    \n",
    "    ##Query again\n",
    "    res = es.scroll(scroll_id=sid, scroll='2m')\n",
    "    # Update the scroll ID\n",
    "    first = False\n",
    "    sid = res['_scroll_id']\n",
    "    scroll_size = len(res['hits']['hits'])\n",
    "    \n",
    "print(\"END\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "STARTED : Wed Jan 27 14:24:45 2021\n",
      "END : Wed Jan 27 17:56:31 2021\n",
      "TIME ELAPSED:211.76080036560694 MINUTES\n"
     ]
    }
   ],
   "source": [
    "done = time()\n",
    "elapsed = done - start\n",
    "print(\"*\"*20)\n",
    "print(\"STARTED : \"+ctime(start) )\n",
    "print(\"END : \"+ctime(done) )\n",
    "print(\"TIME ELAPSED:\" + str(elapsed/60) + \" MINUTES\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
